{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8cea7b4-3024-4366-94eb-1fdca231bc52",
   "metadata": {},
   "source": [
    "### Data preprocessing for the 1st proactive strategy [PMF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b0bdee-0413-4c21-bc2c-e4629a497b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data preprocessing\n",
    "import os\n",
    "import gc\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import groupby\n",
    "from tabulate import tabulate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6fd00-74c5-4e18-a3ce-9d255194c0f5",
   "metadata": {},
   "source": [
    "#### Loading the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56778d52-9cc6-4703-8834-e39d03c9be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Shape: (7409, 24)\n"
     ]
    }
   ],
   "source": [
    "# Loading the raw dataset\n",
    "data = utils.load_data(\"data/raw/cobot_data.xlsx\", \"xlsx\")\n",
    "print(f'Dataset loaded. Shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98abfdb4-8b5c-40e9-a97a-22719198398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after dropping column \"Num\". Shape: (7409, 23)\n"
     ]
    }
   ],
   "source": [
    "# Dropping the column \"Num\" and renaming a few columns for better readability\n",
    "data = data.drop(columns=\"Num\")\n",
    "print(f'Dataset after dropping column \"Num\". Shape: {data.shape}')\n",
    "\n",
    "data.rename(columns={\n",
    "    'cycle': 'Cycle',\n",
    "    'Robot_ProtectiveStop': 'Robot Protective Stop', \n",
    "    'grip_lost': 'Grip Lost', \n",
    "    'Tool_current': 'Tool Current'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11d9977-a7dc-4bc1-911b-54c044e60e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Timestamp', 'Current_J0', 'Temperature_T0', 'Current_J1', 'Temperature_J1', 'Current_J2', 'Temperature_J2', 'Current_J3', 'Temperature_J3', 'Current_J4', 'Temperature_J4', 'Current_J5', 'Temperature_J5', 'Speed_J0', 'Speed_J1', 'Speed_J2', 'Speed_J3', 'Speed_J4', 'Speed_J5', 'Tool Current', 'Cycle', 'Robot Protective Stop', 'Grip Lost']\n"
     ]
    }
   ],
   "source": [
    "# Checking the columns present in the dataset\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040addcf-6bdd-4cc9-90f7-14c90da6c37f",
   "metadata": {},
   "source": [
    "#### Preprocessing the timestamp feature in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e8f093-e877-45ef-a289-4baeb0bfbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the timestamp feature and get new temporal feature for the dataset\n",
    "def process_timestamp(df):\n",
    "    df[\"Timestamp\"] = pd.to_datetime(\n",
    "        df[\"Timestamp\"].str.strip('\"'),\n",
    "        format=\"%Y-%m-%dT%H:%M:%S.%fZ\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    dropped_rows = df[\"Timestamp\"].isnull().sum()\n",
    "    if dropped_rows > 0:\n",
    "        print(f\"Warning: {dropped_rows} rows dropped due to invalid timestamps.\")\n",
    "        df = df.dropna(subset=[\"Timestamp\"])\n",
    "    \n",
    "    df = df.sort_values(by=\"Timestamp\", ascending=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"Hour\"] = df[\"Timestamp\"].dt.hour\n",
    "    df[\"Minute\"] = df[\"Timestamp\"].dt.minute\n",
    "    df[\"Second\"] = df[\"Timestamp\"].dt.second\n",
    "    df['Time of Day'] = (df['Hour'] * 3600) + (df['Minute'] * 60) + df['Second']\n",
    "    return df.drop(columns=[\"Timestamp\", \"Hour\", \"Minute\", \"Second\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e3e22b-38cc-47c5-ac14-831c7d840091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after creating new temporal feature using column \"Timestamp\". Shape: (7409, 23)\n"
     ]
    }
   ],
   "source": [
    "# Creating a new temporal feature using timestamp\n",
    "data = process_timestamp(data)\n",
    "print(f'Dataset after creating new temporal feature using column \"Timestamp\". Shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735c8955-1de2-480b-b4d3-65df3618f80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features identified: ['Current_J0', 'Current_J1', 'Current_J2', 'Current_J3', 'Current_J4', 'Current_J5', 'Cycle', 'Speed_J0', 'Speed_J1', 'Speed_J2', 'Speed_J3', 'Speed_J4', 'Speed_J5', 'Temperature_J1', 'Temperature_J2', 'Temperature_J3', 'Temperature_J4', 'Temperature_J5', 'Temperature_T0', 'Time of Day']\n",
      "Features Count: 20\n"
     ]
    }
   ],
   "source": [
    "# Selecting the features needed from the dataset\n",
    "features = data.columns.difference([\"Robot Protective Stop\", \"Grip Lost\", \"Tool Current\"]).tolist()\n",
    "print(\"Features identified:\", features)\n",
    "print(\"Features Count:\", len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009b463-31af-41a1-81d1-3522df35dabe",
   "metadata": {},
   "source": [
    "#### Handling missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67fb7ba6-f506-4003-8ab6-eb4c5ab8d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate missing values report for a dataset\n",
    "def generate_missing_values_table(dataset, threshold=0.5):\n",
    "    try:\n",
    "        if not isinstance(dataset, pd.DataFrame):\n",
    "            raise ValueError(\"Input dataset must be a pandas DataFrame.\")\n",
    "\n",
    "        missing_values = dataset.isnull().sum()\n",
    "        missing_percentage = (missing_values / len(dataset)) * 100\n",
    "\n",
    "        summary = pd.DataFrame({\n",
    "            \"Column\": dataset.columns,\n",
    "            \"Missing Values\": missing_values,\n",
    "            \"Missing Percentage (%)\": missing_percentage\n",
    "        })\n",
    "\n",
    "        summary = summary[summary[\"Missing Values\"] > 0].reset_index(drop=True)\n",
    "\n",
    "        null_row_threshold = int(len(dataset.columns) * threshold)\n",
    "        mostly_null_rows = (dataset.isnull().sum(axis=1) > null_row_threshold).sum()\n",
    "\n",
    "        output = []\n",
    "        if summary.empty:\n",
    "            output.append(\"No missing values found in the dataset.\")\n",
    "        else:\n",
    "            output.append(tabulate(summary, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n",
    "\n",
    "        output.append(f\"\\nRows with >{int(threshold * 100)}% null values: {mostly_null_rows}\")\n",
    "\n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d433bacf-d92b-4827-8e18-aa6e3a6493b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+--------------------------+\n",
      "| Column                |   Missing Values |   Missing Percentage (%) |\n",
      "+=======================+==================+==========================+\n",
      "| Current_J0            |               46 |                 0.620867 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Temperature_T0        |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Current_J1            |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Temperature_J1        |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Current_J2            |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Temperature_J2        |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Current_J3            |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Temperature_J3        |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Current_J4            |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Temperature_J4        |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Current_J5            |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Temperature_J5        |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Speed_J0              |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Speed_J1              |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Speed_J2              |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Speed_J3              |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Speed_J4              |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Speed_J5              |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Tool Current          |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "| Robot Protective Stop |               54 |                 0.728843 |\n",
      "+-----------------------+------------------+--------------------------+\n",
      "\n",
      "Rows with >50% null values: 54\n"
     ]
    }
   ],
   "source": [
    "# Checking if the missing values are present in the dataset\n",
    "print(generate_missing_values_table(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a62eeb3-c933-4c91-b571-18cd757a3890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in the dataset.\n",
      "\n",
      "Rows with >50% null values: 0\n"
     ]
    }
   ],
   "source": [
    "# Handling the missing values in the target values\n",
    "data[\"Robot Protective Stop\"] = data[\"Robot Protective Stop\"].fillna(\"FALSE\" if (data[\"Grip Lost\"] == \"FALSE\").any() else data[\"Robot Protective Stop\"])\n",
    "data[\"Grip Lost\"] = data[\"Grip Lost\"].fillna(\"FALSE\" if (data[\"Robot Protective Stop\"] == \"FALSE\").any() else data[\"Grip Lost\"])\n",
    "\n",
    "# Handling the missing values in the features\n",
    "data = data.ffill().bfill()\n",
    "data[\"Grip Lost\"] = data[\"Grip Lost\"].astype(int)\n",
    "\n",
    "# Checking if the missing values are still present in the dataset\n",
    "print(generate_missing_values_table(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "305f9844-dede-45bf-8e16-05ad5ecb1708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: data/processed\\handling_missing.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the dataset\n",
    "utils.save_data_csv(data, \"data/processed\", \"handling_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7271772d-08de-4ad1-a62a-27069502df02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing up memory\n",
    "del data\n",
    "del features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9994a17-af26-4db7-a8ff-8c3c128c35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data = utils.load_data(\"data/processed/handling_missing.csv\", \"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aaa492-9e09-4c9c-bfb0-0a67bd778185",
   "metadata": {},
   "source": [
    "#### Handling outliers in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe8bf3bd-af7d-4b7e-ae55-f373b30726d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features identified: ['Current_J0', 'Current_J1', 'Current_J2', 'Current_J3', 'Current_J4', 'Current_J5', 'Cycle', 'Speed_J0', 'Speed_J1', 'Speed_J2', 'Speed_J3', 'Speed_J4', 'Speed_J5', 'Temperature_J1', 'Temperature_J2', 'Temperature_J3', 'Temperature_J4', 'Temperature_J5', 'Temperature_T0', 'Time of Day']\n",
      "Features Count: 20\n"
     ]
    }
   ],
   "source": [
    "# Selecting the features from the dataset\n",
    "features = data.columns.difference([\"Robot Protective Stop\", \"Grip Lost\", \"Tool Current\"]).tolist()\n",
    "print(\"Features identified:\", features)\n",
    "print(\"Features Count:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d67f678-ba42-41ba-a2c4-aaf2a6347b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle outliers present in the dataset\n",
    "def handle_outliers(df, features, threshold):\n",
    "    try:\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(\"The input dataset must be a pandas DataFrame.\")\n",
    "\n",
    "        if not isinstance(features, list):\n",
    "            raise TypeError(\"The features parameter must be a list of feature names.\")\n",
    "\n",
    "        for feature in features:\n",
    "            if feature not in df.columns:\n",
    "                raise ValueError(f\"Feature '{feature}' is not present in the dataset.\")\n",
    "\n",
    "        for feature in features:\n",
    "            z_scores = (df[feature] - df[feature].mean()) / df[feature].std()\n",
    "            df[feature] = np.where(z_scores.abs() > threshold, np.nan, df[feature])\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d700a0-281c-42bd-b04b-598602cc30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------------+\n",
      "| Column     |   Missing Values |   Missing Percentage (%) |\n",
      "+============+==================+==========================+\n",
      "| Current_J0 |              209 |                 2.82089  |\n",
      "+------------+------------------+--------------------------+\n",
      "| Current_J1 |              155 |                 2.09205  |\n",
      "+------------+------------------+--------------------------+\n",
      "| Current_J2 |              159 |                 2.14604  |\n",
      "+------------+------------------+--------------------------+\n",
      "| Current_J3 |              197 |                 2.65893  |\n",
      "+------------+------------------+--------------------------+\n",
      "| Current_J4 |              193 |                 2.60494  |\n",
      "+------------+------------------+--------------------------+\n",
      "| Current_J5 |               12 |                 0.161965 |\n",
      "+------------+------------------+--------------------------+\n",
      "| Speed_J0   |              242 |                 3.2663   |\n",
      "+------------+------------------+--------------------------+\n",
      "| Speed_J1   |              174 |                 2.3485   |\n",
      "+------------+------------------+--------------------------+\n",
      "| Speed_J2   |              552 |                 7.4504   |\n",
      "+------------+------------------+--------------------------+\n",
      "| Speed_J3   |              152 |                 2.05156  |\n",
      "+------------+------------------+--------------------------+\n",
      "| Speed_J4   |              247 |                 3.33378  |\n",
      "+------------+------------------+--------------------------+\n",
      "| Speed_J5   |              163 |                 2.20003  |\n",
      "+------------+------------------+--------------------------+\n",
      "\n",
      "Rows with >30% null values: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking for outliers in the dataset\n",
    "data = handle_outliers(data, features, threshold=3)\n",
    "print(generate_missing_values_table(data, threshold=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6b1ae9d-b8d8-4920-8f3c-12e863652bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in the dataset.\n",
      "\n",
      "Rows with >30% null values: 0\n"
     ]
    }
   ],
   "source": [
    "# Handling the outliers present in the dataset\n",
    "data = data.ffill().bfill()\n",
    "print(generate_missing_values_table(data, threshold=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f5807c5-880c-4e5d-af9b-aeb0bed266f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing outliers. Shape: (7409, 23)\n",
      "Dataset saved: data/processed\\outliers_removed.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the initial processed dataset\n",
    "print(f'Dataset after removing outliers. Shape: {data.shape}')\n",
    "utils.save_data_csv(data, \"data/processed\", \"outliers_removed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "587339a2-5604-4eb0-93d1-d5055ff33888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing up memory\n",
    "del data\n",
    "del features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef696e01-c384-4574-99d4-255425f9afa5",
   "metadata": {},
   "source": [
    "#### Adding interaction features to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edbb82b3-a037-4f04-98dc-8fd78a85f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the initial processed dataset\n",
    "data = utils.load_data(\"data/processed/outliers_removed.csv\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58568365-6b97-4c72-a0d2-5e91271218b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the cycle time of each operational cycle and also the number of occurrences from the feature \"Cycle\" present in the dataset\n",
    "cycle_summary = data[[\"Time of Day\", \"Cycle\"]].groupby('Cycle').agg(\n",
    "    cycle_time=('Time of Day', lambda x: x.max() - x.min()),\n",
    "    occurrences=('Cycle', 'size')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e48973a9-14d4-48e9-a0a2-c28203c7e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: data/processed\\cycle_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the results into a dataset for future usage\n",
    "utils.save_data_csv(cycle_summary, \"data/processed\", \"cycle_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18f2f40c-8bb4-47b3-a4be-557f094d5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features called \"Sin Time\" and \"Cos Time\" using the created temporal feature \"Time of Day\"\n",
    "data['Sin Time'] = np.sin(2 * np.pi * data[\"Time of Day\"] / 86400)\n",
    "data['Cos Time'] = np.cos(2 * np.pi * data[\"Time of Day\"] / 86400)\n",
    "\n",
    "# Creating a new interaction feature called \"Time Phase\" in the range -pie to pie using the created features \"Sin Time\" and \"Cos Time\"\n",
    "data['Time Phase'] = np.arctan2(data['Sin Time'], data['Cos Time'])\n",
    "\n",
    "# Normalizing the column to 0 to 2pie for model training\n",
    "data['Time Phase'] = (data['Time Phase'] + 2 * np.pi) % (2 * np.pi)\n",
    "\n",
    "# Creating a new interaction feature called \"Cycle Time\" using the operational cycle count feature present in the dataset\n",
    "data['Cycle Time'] = data.groupby('Cycle')['Cycle'].transform('size')\n",
    "data = data.drop(columns=[\"Time of Day\", \"Sin Time\", \"Cos Time\", \"Cycle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c8b00d3-e5a1-40bf-934e-6963bfb77c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new interaction features called \"Average Temperature\" and \"Gradient Temperature\" using the temperature joints featureqs\n",
    "data['Average Temperature'] = data[[f'Temperature_J{i}' for i in range(1, 6)] + ['Temperature_T0']].mean(axis=1)\n",
    "data['Gradient Temperature'] = (data[[f'Temperature_J{i}' for i in range(1, 6)] + ['Temperature_T0']].max(axis=1) - \n",
    "                                data[[f'Temperature_J{i}' for i in range(1, 6)] + ['Temperature_T0']].min(axis=1))\n",
    "\n",
    "# Creating new interaction features that define the direction of current and speeds from each joint in the dataset\n",
    "for i in range(6):\n",
    "    data[f'Speed_Direction_J{i}'] = np.sign(data[f'Speed_J{i}'])\n",
    "    data[f'Speed_J{i}'] = np.abs(data[f'Speed_J{i}'])\n",
    "    \n",
    "    data[f'Current_Direction_J{i}'] = np.sign(data[f'Current_J{i}'])\n",
    "    data[f'Current_J{i}'] = np.abs(data[f'Current_J{i}'])\n",
    "\n",
    "# Creating a new feature called \"Load Imbalance\" using the current joints values present in the dataset\n",
    "data['Load Imbalance'] = (\n",
    "    data[[f'Current_J{i}' for i in range(0, 6)]].max(axis=1) - \n",
    "    data[[f'Current_J{i}' for i in range(0, 6)]].min(axis=1)) / data[[f'Current_J{i}' for i in range(0, 6)]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31263b80-4998-493a-b05a-2b3596a9607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after creating interaction features. Shape: (7409, 38)\n",
      "Features identified: ['Average Temperature', 'Current_Direction_J0', 'Current_Direction_J1', 'Current_Direction_J2', 'Current_Direction_J3', 'Current_Direction_J4', 'Current_Direction_J5', 'Current_J0', 'Current_J1', 'Current_J2', 'Current_J3', 'Current_J4', 'Current_J5', 'Cycle Time', 'Gradient Temperature', 'Load Imbalance', 'Speed_Direction_J0', 'Speed_Direction_J1', 'Speed_Direction_J2', 'Speed_Direction_J3', 'Speed_Direction_J4', 'Speed_Direction_J5', 'Speed_J0', 'Speed_J1', 'Speed_J2', 'Speed_J3', 'Speed_J4', 'Speed_J5', 'Temperature_J1', 'Temperature_J2', 'Temperature_J3', 'Temperature_J4', 'Temperature_J5', 'Temperature_T0', 'Time Phase']\n",
      "Features Count: 35\n"
     ]
    }
   ],
   "source": [
    "# Selecting the newly added interaction features from the dataset\n",
    "print(f'Dataset after creating interaction features. Shape: {data.shape}')\n",
    "features = data.columns.difference([\"Robot Protective Stop\", \"Grip Lost\", \"Tool Current\"]).tolist()\n",
    "print(\"Features identified:\", features)\n",
    "print(\"Features Count:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "439e0d36-7fc5-4182-a60c-64512dfc7618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: data/processed\\interaction_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the newly added interaction features dataset\n",
    "utils.save_data_csv(data, \"data/processed\", \"interaction_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea68ea59-d5c1-4fbe-9a7b-60af5356118e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing up memory\n",
    "del data\n",
    "del features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62b81e-3ee6-4746-8b87-c9c9586820c1",
   "metadata": {},
   "source": [
    "### Saving the processed dataset with ordered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1af9cf85-db83-433d-8888-80f0faadc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the interaction features dataset\n",
    "data = utils.load_data(\"data/processed/interaction_features.csv\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63168711-02dd-42ea-bc00-4757614b4a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features identified: ['Average Temperature', 'Current_Direction_J0', 'Current_Direction_J1', 'Current_Direction_J2', 'Current_Direction_J3', 'Current_Direction_J4', 'Current_Direction_J5', 'Current_J0', 'Current_J1', 'Current_J2', 'Current_J3', 'Current_J4', 'Current_J5', 'Cycle Time', 'Gradient Temperature', 'Load Imbalance', 'Speed_Direction_J0', 'Speed_Direction_J1', 'Speed_Direction_J2', 'Speed_Direction_J3', 'Speed_Direction_J4', 'Speed_Direction_J5', 'Speed_J0', 'Speed_J1', 'Speed_J2', 'Speed_J3', 'Speed_J4', 'Speed_J5', 'Temperature_J1', 'Temperature_J2', 'Temperature_J3', 'Temperature_J4', 'Temperature_J5', 'Temperature_T0', 'Time Phase']\n",
      "Features Count: 35\n"
     ]
    }
   ],
   "source": [
    "# Selecting the features from the loaded dataset\n",
    "features = data.columns.difference([\"Robot Protective Stop\", \"Grip Lost\", \"Tool Current\"]).tolist()\n",
    "print(\"Features identified:\", features)\n",
    "print(\"Features Count:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c2c1c5d-3a4f-4be5-bb0e-8bae299e0fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after reordering. Shape: (7409, 38)\n",
      "['Time Phase', 'Cycle Time', 'Current_J0', 'Current_J1', 'Current_J2', 'Current_J3', 'Current_J4', 'Current_J5', 'Current_Direction_J0', 'Current_Direction_J1', 'Current_Direction_J2', 'Current_Direction_J3', 'Current_Direction_J4', 'Current_Direction_J5', 'Speed_J0', 'Speed_J1', 'Speed_J2', 'Speed_J3', 'Speed_J4', 'Speed_J5', 'Speed_Direction_J0', 'Speed_Direction_J1', 'Speed_Direction_J2', 'Speed_Direction_J3', 'Speed_Direction_J4', 'Speed_Direction_J5', 'Temperature_T0', 'Temperature_J1', 'Temperature_J2', 'Temperature_J3', 'Temperature_J4', 'Temperature_J5', 'Average Temperature', 'Gradient Temperature', 'Load Imbalance', 'Robot Protective Stop', 'Grip Lost', 'Tool Current']\n"
     ]
    }
   ],
   "source": [
    "# Reordering the dataset features and target variables for readability\n",
    "new_order = [\n",
    "    'Time Phase', 'Cycle Time',\n",
    "    *[f'Current_J{i}' for i in range(6)],\n",
    "    *[f'Current_Direction_J{i}' for i in range(6)],\n",
    "    *[f'Speed_J{i}' for i in range(6)],\n",
    "    *[f'Speed_Direction_J{i}' for i in range(6)],\n",
    "    'Temperature_T0', *[f'Temperature_J{i}' for i in range(1, 6)],\n",
    "    'Average Temperature', 'Gradient Temperature', 'Load Imbalance',\n",
    "    'Robot Protective Stop', 'Grip Lost', 'Tool Current'\n",
    "]\n",
    "\n",
    "# Arranging the data frame according to the newly defined order\n",
    "data = data[new_order]\n",
    "print(f'Dataset after reordering. Shape: {data.shape}')\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8607ffee-dd50-47f9-adcf-6856f05e2f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: data/processed\\processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the ordered and processed dataset\n",
    "utils.save_data_csv(data, \"data/processed\", \"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a42a157d-eece-4fee-833a-33ad54406726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing up memory\n",
    "del data\n",
    "del features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b0e97-0b90-40b5-8d3e-40e3806b0a99",
   "metadata": {},
   "source": [
    "#### Adding rolling features to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3deef0cd-53f5-4d1b-9fd7-60f41c59ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the ordered and processed dataset\n",
    "data = utils.load_data(\"data/processed/processed_data.csv\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4c6e05e-7d96-480d-822d-b799512fbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and add rolling features to the processed dataset\n",
    "def add_rolling_features(df, features, window_size):\n",
    "    try:\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"The dataset must be a pandas DataFrame.\")\n",
    "        if not isinstance(features, list) or not all(isinstance(i, str) for i in features):\n",
    "            raise ValueError(\"Features must be a list of column names (strings).\")\n",
    "        if not isinstance(window_size, int) or window_size <= 0:\n",
    "            raise ValueError(\"Window size must be a positive integer.\")\n",
    "\n",
    "        missing_features = [feature for feature in features if feature not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"The following features are missing from the dataset: {', '.join(missing_features)}\")\n",
    "\n",
    "        rolling_features = {}\n",
    "        for feature in features:\n",
    "            rolling_features[f\"{feature}_rolling_mean\"] = df[feature].rolling(window=window_size, min_periods=1).mean()\n",
    "            rolling_features[f\"{feature}_rolling_std\"] = df[feature].rolling(window=window_size, min_periods=1).std()\n",
    "\n",
    "        rolling_df = df.copy()\n",
    "        for key, value in rolling_features.items():\n",
    "            rolling_df[key] = value\n",
    "\n",
    "        return rolling_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10ac51f9-36f1-4b1e-bbe3-2a8fb5fb249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features for adding rolling features\n",
    "features_rll = [\n",
    "    *[f'Current_J{i}' for i in range(6)],\n",
    "    *[f'Speed_J{i}' for i in range(6)], \n",
    "    'Temperature_T0', *[f'Temperature_J{i}' for i in range(1, 6)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aecad99f-1cba-469e-a3f2-2580dbf1d047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter Mean (average cycle interval length) of operational cycle -> 30\n"
     ]
    }
   ],
   "source": [
    "# Loading the cycle summary dataset\n",
    "cycle_summary = utils.load_data(\"data/processed/cycle_summary.csv\", \"csv\")\n",
    "\n",
    "# Calculating the inter mean (average cycle interval length)\n",
    "sequence_length = int(np.mean(cycle_summary[\"occurrences\"]))\n",
    "\n",
    "# Saving the sequence length\n",
    "with open(\"data/processed/sequence_length.txt\", \"w\") as file:\n",
    "    file.write(str(sequence_length))\n",
    "\n",
    "print(f\"Inter Mean (average cycle interval length) of operational cycle -> {sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8381fd6-a2c7-4f8e-9b6f-2f17881d48f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - After adding rolling features. Shape: (7409, 74)\n",
      "Features identified: ['Average Temperature', 'Current_Direction_J0', 'Current_Direction_J1', 'Current_Direction_J2', 'Current_Direction_J3', 'Current_Direction_J4', 'Current_Direction_J5', 'Current_J0', 'Current_J0_rolling_mean', 'Current_J0_rolling_std', 'Current_J1', 'Current_J1_rolling_mean', 'Current_J1_rolling_std', 'Current_J2', 'Current_J2_rolling_mean', 'Current_J2_rolling_std', 'Current_J3', 'Current_J3_rolling_mean', 'Current_J3_rolling_std', 'Current_J4', 'Current_J4_rolling_mean', 'Current_J4_rolling_std', 'Current_J5', 'Current_J5_rolling_mean', 'Current_J5_rolling_std', 'Cycle Time', 'Gradient Temperature', 'Load Imbalance', 'Speed_Direction_J0', 'Speed_Direction_J1', 'Speed_Direction_J2', 'Speed_Direction_J3', 'Speed_Direction_J4', 'Speed_Direction_J5', 'Speed_J0', 'Speed_J0_rolling_mean', 'Speed_J0_rolling_std', 'Speed_J1', 'Speed_J1_rolling_mean', 'Speed_J1_rolling_std', 'Speed_J2', 'Speed_J2_rolling_mean', 'Speed_J2_rolling_std', 'Speed_J3', 'Speed_J3_rolling_mean', 'Speed_J3_rolling_std', 'Speed_J4', 'Speed_J4_rolling_mean', 'Speed_J4_rolling_std', 'Speed_J5', 'Speed_J5_rolling_mean', 'Speed_J5_rolling_std', 'Temperature_J1', 'Temperature_J1_rolling_mean', 'Temperature_J1_rolling_std', 'Temperature_J2', 'Temperature_J2_rolling_mean', 'Temperature_J2_rolling_std', 'Temperature_J3', 'Temperature_J3_rolling_mean', 'Temperature_J3_rolling_std', 'Temperature_J4', 'Temperature_J4_rolling_mean', 'Temperature_J4_rolling_std', 'Temperature_J5', 'Temperature_J5_rolling_mean', 'Temperature_J5_rolling_std', 'Temperature_T0', 'Temperature_T0_rolling_mean', 'Temperature_T0_rolling_std', 'Time Phase']\n",
      "Features Count: 71\n"
     ]
    }
   ],
   "source": [
    "# Adding rolling features to the dataset\n",
    "data = add_rolling_features(data, features_rll, sequence_length).ffill().bfill()\n",
    "print(f'Dataset - After adding rolling features. Shape: {data.shape}')\n",
    "\n",
    "# Selecting the features from the newly added rolling features dataset\n",
    "features = data.columns.difference([\"Robot Protective Stop\", \"Grip Lost\", \"Tool Current\"]).tolist()\n",
    "print(\"Features identified:\", features)\n",
    "print(\"Features Count:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6e244dd-7db3-47a5-9022-89c8893ff509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: data/processed\\rolling_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the rolling features dataset\n",
    "utils.save_data_csv(data, \"data/processed\", \"rolling_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80c94a6a-8309-41d0-bf96-10dc9c005645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing up memory\n",
    "del data\n",
    "del features_rll\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87976124-7a7c-4a1f-b4ee-d471935b80af",
   "metadata": {},
   "source": [
    "#### Spitting the dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8996594e-00eb-4fd8-b6b0-d0a0571b7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the rolling features dataset\n",
    "data = utils.load_data(\"data/processed/rolling_features.csv\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ef81f7b-54b9-4353-b4d1-d8d96f04402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 3 separate datasets for each target variable\n",
    "data_rb = data[data.columns.difference([\"Grip Lost\", \"Tool Current\"]).tolist()].copy()\n",
    "features_rb = data_rb.columns.difference(['Robot Protective Stop']).tolist()\n",
    "target_rb = \"Robot Protective Stop\"\n",
    "\n",
    "data_gl = data[data.columns.difference([\"Robot Protective Stop\", \"Tool Current\"]).tolist()].copy()\n",
    "features_gl = data_gl.columns.difference(['Grip Lost']).tolist()\n",
    "target_gl = \"Grip Lost\"\n",
    "\n",
    "data_tc = data[data.columns.difference([\"Robot Protective Stop\", \"Grip Lost\"]).tolist()].copy()\n",
    "features_tc = data_tc.columns.difference(['Tool Current']).tolist()\n",
    "target_tc = \"Tool Current\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bb29d29-3e51-4b73-84d4-4089ac2219a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable \"Robot Protective Stop\" dataset - Train feature Shape: (4772, 71), Train target Shape: (4772,)\n",
      "Target variable \"Robot Protective Stop\" dataset - Validation features Shape: (1737, 71), Validation target Shape: (1737,)\n",
      "Target variable \"Robot Protective Stop\" dataset - Test features Shape: (899, 71), Test target Shape: (899,)\n",
      "\n",
      "Target variable \"Grip Lost\" dataset - Train features Shape: (4772, 71), Train target Shape: (4772,)\n",
      "Target variable \"Grip Lost\" dataset - Validation features Shape: (1737, 71), Validation target Shape: (1737,)\n",
      "Target variable \"Grip Lost\" dataset - Test features Shape: (899, 71), Test target Shape: (899,)\n",
      "\n",
      "Target variable \"Tool Current\" dataset - Train features Shape: (4772, 71), Train target Shape: (4772,)\n",
      "Target variable \"Tool Current\" dataset - Validation features Shape: (1737, 71), Validation target Shape: (1737,)\n",
      "Target variable \"Tool Current\" dataset - Test features Shape: (899, 71), Test target Shape: (899,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the datasets into train, test and validation sets for model training and evaluation\n",
    "trd_rb, trl_rb, vad_rb, val_rb, ted_rb, tel_rb = utils.split_data(data_rb, features_rb, target_rb)\n",
    "print(f'Target variable \"Robot Protective Stop\" dataset - Train feature Shape: {trd_rb.shape}, Train target Shape: {trl_rb.shape}')\n",
    "print(f'Target variable \"Robot Protective Stop\" dataset - Validation features Shape: {vad_rb.shape}, Validation target Shape: {val_rb.shape}')\n",
    "print(f'Target variable \"Robot Protective Stop\" dataset - Test features Shape: {ted_rb.shape}, Test target Shape: {tel_rb.shape}')\n",
    "\n",
    "trd_gl, trl_gl, vad_gl, val_gl, ted_gl, tel_gl = utils.split_data(data_gl, features_gl, target_gl)\n",
    "print(f'\\nTarget variable \"Grip Lost\" dataset - Train features Shape: {trd_gl.shape}, Train target Shape: {trl_gl.shape}')\n",
    "print(f'Target variable \"Grip Lost\" dataset - Validation features Shape: {vad_gl.shape}, Validation target Shape: {val_gl.shape}')\n",
    "print(f'Target variable \"Grip Lost\" dataset - Test features Shape: {ted_gl.shape}, Test target Shape: {tel_gl.shape}')\n",
    "\n",
    "trd_tc, trl_tc, vad_tc, val_tc, ted_tc, tel_tc = utils.split_data(data_tc, features_tc, target_tc)\n",
    "print(f'\\nTarget variable \"Tool Current\" dataset - Train features Shape: {trd_tc.shape}, Train target Shape: {trl_tc.shape}')\n",
    "print(f'Target variable \"Tool Current\" dataset - Validation features Shape: {vad_tc.shape}, Validation target Shape: {val_tc.shape}')\n",
    "print(f'Target variable \"Tool Current\" dataset - Test features Shape: {ted_tc.shape}, Test target Shape: {tel_tc.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f02c5cc7-0670-4365-a3b9-0ccaec34d56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: data/processed/rb/train\\train_data_rb.csv\n",
      "Dataset saved: data/processed/rb/train\\train_labels_rb.csv\n",
      "Dataset saved: data/processed/rb/valid\\valid_data_rb.csv\n",
      "Dataset saved: data/processed/rb/valid\\valid_labels_rb.csv\n",
      "Dataset saved: data/processed/rb/test\\test_data_rb.csv\n",
      "Dataset saved: data/processed/rb/test\\test_labels_rb.csv\n",
      "Dataset saved: data/processed/gl/train\\train_data_gl.csv\n",
      "Dataset saved: data/processed/gl/train\\train_labels_gl.csv\n",
      "Dataset saved: data/processed/gl/valid\\valid_data_gl.csv\n",
      "Dataset saved: data/processed/gl/valid\\valid_labels_gl.csv\n",
      "Dataset saved: data/processed/gl/test\\test_data_gl.csv\n",
      "Dataset saved: data/processed/gl/test\\test_labels_gl.csv\n",
      "Dataset saved: data/processed/tc/train\\train_data_tc.csv\n",
      "Dataset saved: data/processed/tc/train\\train_labels_tc.csv\n",
      "Dataset saved: data/processed/tc/valid\\valid_data_tc.csv\n",
      "Dataset saved: data/processed/tc/valid\\valid_labels_tc.csv\n",
      "Dataset saved: data/processed/tc/test\\test_data_tc.csv\n",
      "Dataset saved: data/processed/tc/test\\test_labels_tc.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the 3 created datasets for later use\n",
    "datasets = {\n",
    "    \"rb\": {\"train\": (trd_rb, trl_rb), \"valid\": (vad_rb, val_rb), \"test\": (ted_rb, tel_rb)},\n",
    "    \"gl\": {\"train\": (trd_gl, trl_gl), \"valid\": (vad_gl, val_gl), \"test\": (ted_gl, tel_gl)},\n",
    "    \"tc\": {\"train\": (trd_tc, trl_tc), \"valid\": (vad_tc, val_tc), \"test\": (ted_tc, tel_tc)}\n",
    "}\n",
    "\n",
    "for key, splits in datasets.items():\n",
    "    for split, (data, labels) in splits.items():\n",
    "        utils.save_data_csv(data, f\"data/processed/{key}/{split}\", f\"{split}_data_{key}.csv\")\n",
    "        utils.save_data_csv(labels, f\"data/processed/{key}/{split}\", f\"{split}_labels_{key}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5610bc3-97a0-47cd-9ab7-0d02e6ce8a35",
   "metadata": {},
   "source": [
    "#### Applying the time series SMOTE balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a806d2c1-18cc-435f-92f7-9bd280a8c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply tiem series SMOTE on train data and train labels\n",
    "def smote_time_series_balancing(train_data, train_labels):\n",
    "    try:\n",
    "        if not isinstance(train_data, pd.DataFrame):\n",
    "            raise ValueError(\"train_data must be a pandas DataFrame.\")\n",
    "        if not isinstance(train_labels, (pd.Series, np.ndarray, list)):\n",
    "            raise ValueError(\"train_labels must be a pandas Series, numpy array, or list.\")\n",
    "\n",
    "        smote = SMOTE()\n",
    "        balanced_data, balanced_labels = smote.fit_resample(train_data, train_labels)\n",
    "        balanced_data = pd.DataFrame(balanced_data, columns=train_data.columns)\n",
    "        balanced_data['Labels'] = balanced_labels\n",
    "\n",
    "        if 'Time Phase' in balanced_data.columns:\n",
    "            balanced_data.sort_values(by='Time Phase', inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"The dataset must contain 'Time Phase' column to sort.\")\n",
    "\n",
    "        balanced_labels = balanced_data.pop('Labels')\n",
    "        return balanced_data, balanced_labels\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An error occurred during SMOTE analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3fcc745-ca4d-40b7-893d-f8e6f3b0d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before time series SMOTE:\n",
      "Class distribution in 'Robot Protective Stop' target variable: \n",
      "Robot Protective Stop\n",
      "0.0    4630\n",
      "1.0     142\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in 'Grip Lost' target variable: \n",
      "Grip Lost\n",
      "0    4609\n",
      "1     163\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking the class distribution in target variables before applying the time series SMOTE\n",
    "print(\"Before time series SMOTE:\")\n",
    "print(f\"Class distribution in 'Robot Protective Stop' target variable: \\n{trl_rb.value_counts()}\")\n",
    "print(f\"\\nClass distribution in 'Grip Lost' target variable: \\n{trl_gl.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24c1aac0-3b28-4186-9525-7fd88f090177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Robot Protective Stop\" dataset - Features Shape: (4772, 71), Labels Shape: (4772,)\n",
      "\"Grip Lost\" dataset - Features Shape: (4772, 71), Labels Shape: (4772,)\n",
      "\"Robot Protective Stop\" dataset - Features Shape: (9260, 71), Labels Shape: (9260,)\n",
      "\"Grip Lost\" dataset - Features Shape: (9218, 71), Labels Shape: (9218,)\n"
     ]
    }
   ],
   "source": [
    "print(f'\"Robot Protective Stop\" dataset - Features Shape: {trd_rb.shape}, Labels Shape: {trl_rb.shape}')\n",
    "print(f'\"Grip Lost\" dataset - Features Shape: {trd_gl.shape}, Labels Shape: {trl_gl.shape}')\n",
    "\n",
    "# Apply temporal SMOTE to the datasets\n",
    "trd_rb, trl_rb = smote_time_series_balancing(trd_rb, trl_rb)\n",
    "trd_gl, trl_gl = smote_time_series_balancing(trd_gl, trl_gl)\n",
    "\n",
    "print(f'\"Robot Protective Stop\" dataset - Features Shape: {trd_rb.shape}, Labels Shape: {trl_rb.shape}')\n",
    "print(f'\"Grip Lost\" dataset - Features Shape: {trd_gl.shape}, Labels Shape: {trl_gl.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fa329fd-bc66-40c5-8fbd-71042ac0b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After time series SMOTE:\n",
      "Class distribution in 'Robot Protective Stop' target variable: \n",
      "Labels\n",
      "0.0    4630\n",
      "1.0    4630\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in 'Grip Lost' target variable: \n",
      "Labels\n",
      "0    4609\n",
      "1    4609\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking the class distribution in target variables after applying the temporal SMOTE\n",
    "print(\"\\nAfter time series SMOTE:\")\n",
    "print(f\"Class distribution in 'Robot Protective Stop' target variable: \\n{trl_rb.value_counts()}\")\n",
    "print(f\"\\nClass distribution in 'Grip Lost' target variable: \\n{trl_gl.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6700456d-4cce-4416-a78b-1436cfce8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicates\n",
    "def check_duplicates(data, labels):\n",
    "    feature_duplicates = data.duplicated().sum()\n",
    "\n",
    "    print(f\"Number of duplicate rows in features: {feature_duplicates}\")\n",
    "    if feature_duplicates > 0:\n",
    "        print(\"Duplicates are present in the dataset.\")\n",
    "    else:\n",
    "        print(\"No duplicates found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a418fcec-acc6-4715-bced-fa05636222db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in features: 0\n",
      "No duplicates found in the dataset.\n",
      "Number of duplicate rows in features: 0\n",
      "No duplicates found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Checking if duplicates are present in the dataset\n",
    "check_duplicates(trd_rb, trl_rb)\n",
    "check_duplicates(trd_gl, trl_gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4d307ec-fe66-4aec-9c84-6b48520b3557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: data/processed/rb/train\\balanced_train_data_rb.csv\n",
      "Dataset saved: data/processed/rb/train\\balanced_train_labels_rb.csv\n",
      "Dataset saved: data/processed/gl/train\\balanced_train_data_gl.csv\n",
      "Dataset saved: data/processed/gl/train\\balanced_train_labels_gl.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the datasets for later use\n",
    "datasets = {\n",
    "    \"rb\": (trd_rb, trl_rb),\n",
    "    \"gl\": (trd_gl, trl_gl)\n",
    "}\n",
    "\n",
    "for key, (data, labels) in datasets.items():\n",
    "    utils.save_data_csv(data, f\"data/processed/{key}/train\", f\"balanced_train_data_{key}.csv\")\n",
    "    utils.save_data_csv(labels, f\"data/processed/{key}/train\", f\"balanced_train_labels_{key}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbebc23-f194-40c9-a3ec-42f68fa2e2a3",
   "metadata": {},
   "source": [
    "#### Reordering the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7d75a7e-eb41-4cba-8e49-6549a01c7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the new order for the datasets\n",
    "new_order = [\n",
    "    'Time Phase', 'Cycle Time', \n",
    "    *[f'Current_J{i}' for i in range(6)],\n",
    "    *[f'Current_Direction_J{i}' for i in range(6)],\n",
    "    *[f'Current_J{i}_rolling_mean' for i in range(6)], \n",
    "    *[f'Current_J{i}_rolling_std' for i in range(6)], \n",
    "    *[f'Speed_J{i}' for i in range(6)],\n",
    "    *[f'Speed_Direction_J{i}' for i in range(6)],\n",
    "    *[f'Speed_J{i}_rolling_mean' for i in range(6)], \n",
    "    *[f'Speed_J{i}_rolling_std' for i in range(6)], \n",
    "    'Temperature_T0', *[f'Temperature_J{i}' for i in range(1, 6)], \n",
    "    'Temperature_T0_rolling_mean', *[f'Temperature_J{i}_rolling_std' for i in range(1, 6)],\n",
    "    'Temperature_T0_rolling_std', *[f'Temperature_J{i}_rolling_std' for i in range(1, 6)],\n",
    "    'Average Temperature', 'Gradient Temperature', 'Load Imbalance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c5a7480-b43e-486f-b4a7-01bb65834dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering the datasets for easy readability\n",
    "trd_rb = trd_rb[new_order]\n",
    "vad_rb = vad_rb[new_order]\n",
    "ted_rb = ted_rb[new_order]\n",
    "\n",
    "trd_gl = trd_gl[new_order]\n",
    "vad_gl = vad_gl[new_order]\n",
    "ted_gl = ted_gl[new_order]\n",
    "\n",
    "trd_tc = trd_tc[new_order]\n",
    "vad_tc = vad_tc[new_order]\n",
    "ted_tc = ted_tc[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5c4790c-38c8-4858-a4a5-c6e9b7885ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: data/processed/rb/train\\scaled_train_data_rb.csv\n",
      "Dataset saved: data/processed/rb/valid\\scaled_valid_data_rb.csv\n",
      "Dataset saved: data/processed/rb/test\\scaled_test_data_rb.csv\n",
      "Dataset saved: data/processed/gl/train\\scaled_train_data_gl.csv\n",
      "Dataset saved: data/processed/gl/valid\\scaled_valid_data_gl.csv\n",
      "Dataset saved: data/processed/gl/test\\scaled_test_data_gl.csv\n",
      "Dataset saved: data/processed/tc/train\\scaled_train_data_tc.csv\n",
      "Dataset saved: data/processed/tc/valid\\scaled_valid_data_tc.csv\n",
      "Dataset saved: data/processed/tc/test\\scaled_test_data_tc.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the datasets for model training and evaluation\n",
    "datasets = {\n",
    "    \"rb\": {\"train\": trd_rb, \"valid\": vad_rb, \"test\": ted_rb},\n",
    "    \"gl\": {\"train\": trd_gl, \"valid\": vad_gl, \"test\": ted_gl},\n",
    "    \"tc\": {\"train\": trd_tc, \"valid\": vad_tc, \"test\": ted_tc}\n",
    "}\n",
    "\n",
    "for key, splits in datasets.items():\n",
    "    for split, data in splits.items():\n",
    "        utils.save_data_csv(data, f\"data/processed/{key}/{split}\", f\"scaled_{split}_data_{key}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d67c1-1755-4f9c-ae2c-928fc544c0f2",
   "metadata": {},
   "source": [
    "#### Creating sequences from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "987f82f3-beaf-4aaa-9550-b8859c11782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sequence length value which was calculated earlier\n",
    "with open(\"data/processed/sequence_length.txt\", \"r\") as file:\n",
    "    sequence_length = int(file.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1d849a2-1060-47a3-b182-c524e91f9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sequences for training, validation, and test datasets\n",
    "trd_rb, trl_rb = utils.create_sequences(trd_rb, trl_rb, sequence_length, \"classification\")\n",
    "vad_rb, val_rb = utils.create_sequences(vad_rb, val_rb, sequence_length, \"classification\")\n",
    "ted_rb, tel_rb = utils.create_sequences(ted_rb, tel_rb, sequence_length, \"classification\")\n",
    "\n",
    "trd_gl, trl_gl = utils.create_sequences(trd_gl, trl_gl, sequence_length, \"classification\")\n",
    "vad_gl, val_gl = utils.create_sequences(vad_gl, val_gl, sequence_length, \"classification\")\n",
    "ted_gl, tel_gl = utils.create_sequences(ted_gl, tel_gl, sequence_length, \"classification\")\n",
    "\n",
    "trd_tc, trl_tc = utils.create_sequences(trd_tc, trl_tc, sequence_length, \"regression\")\n",
    "vad_tc, val_tc = utils.create_sequences(vad_tc, val_tc, sequence_length, \"regression\")\n",
    "ted_tc, tel_tc = utils.create_sequences(ted_tc, tel_tc, sequence_length, \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14b2b3e5-0573-42c3-8c18-bcd85c7b26f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data \"Robot Protective Stop\": (9230, 30, 71), Train labels \"Robot Protective Stop\": (9230,)\n",
      "Valid data \"Robot Protective Stop\": (1707, 30, 71), Valid labels \"Robot Protective Stop\": (1707,)\n",
      "Test data \"Robot Protective Stop\": (869, 30, 71), Test labels \"Robot Protective Stop\": (869,)\n",
      "Train data \"Grip Lost\": (9188, 30, 71), Train labels \"Grip Lost\": (9188,)\n",
      "Valid data \"Grip Lost\": (1707, 30, 71), Valid labels \"Grip Lost\": (1707,)\n",
      "Test data \"Grip Lost\": (869, 30, 71), Test labels \"Grip Lost\": (869,)\n",
      "Train data \"Tool Current\": (4742, 30, 71), Train labels \"Tool Current\": (4742,)\n",
      "Valid data \"Tool Current\": (1707, 30, 71), Valid labels \"Tool Current\": (1707,)\n",
      "Test data \"Tool Current\": (869, 30, 71), Test labels \"Tool Current\": (869,)\n"
     ]
    }
   ],
   "source": [
    "# Checking if the sequences are created properly or not\n",
    "datasets = {\n",
    "    'Robot Protective Stop': [trd_rb, trl_rb, vad_rb, val_rb, ted_rb, tel_rb],\n",
    "    'Grip Lost': [trd_gl, trl_gl, vad_gl, val_gl, ted_gl, tel_gl],\n",
    "    'Tool Current': [trd_tc, trl_tc, vad_tc, val_tc, ted_tc, tel_tc]\n",
    "}\n",
    "\n",
    "for label, (x_train, y_train, x_valid, y_valid, x_test, y_test) in datasets.items():\n",
    "    print(f\"Train data \\\"{label}\\\": {x_train.shape}, Train labels \\\"{label}\\\": {y_train.shape}\")\n",
    "    print(f\"Valid data \\\"{label}\\\": {x_valid.shape}, Valid labels \\\"{label}\\\": {y_valid.shape}\")\n",
    "    print(f\"Test data \\\"{label}\\\": {x_test.shape}, Test labels \\\"{label}\\\": {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e783cde2-128a-4839-ae1c-b413fd94a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sequences to: data\\processed\\rb\\train\\sequences\n",
      "Saved sequences to: data\\processed\\rb\\valid\\sequences\n",
      "Saved sequences to: data\\processed\\rb\\test\\sequences\n",
      "Saved sequences to: data\\processed\\gl\\train\\sequences\n",
      "Saved sequences to: data\\processed\\gl\\valid\\sequences\n",
      "Saved sequences to: data\\processed\\gl\\test\\sequences\n",
      "Saved sequences to: data\\processed\\tc\\train\\sequences\n",
      "Saved sequences to: data\\processed\\tc\\valid\\sequences\n",
      "Saved sequences to: data\\processed\\tc\\test\\sequences\n"
     ]
    }
   ],
   "source": [
    "# Saving the numpy arrays for model training and evaluation\n",
    "arrays = {\n",
    "    \"rb\": {\"train\": (trd_rb, trl_rb), \"valid\": (vad_rb, val_rb), \"test\": (ted_rb, tel_rb)},\n",
    "    \"gl\": {\"train\": (trd_gl, trl_gl), \"valid\": (vad_gl, val_gl), \"test\": (ted_gl, tel_gl)},\n",
    "    \"tc\": {\"train\": (trd_tc, trl_tc), \"valid\": (vad_tc, val_tc), \"test\": (ted_tc, tel_tc)}\n",
    "}\n",
    "\n",
    "for key, splits in arrays.items():\n",
    "    for split, data in splits.items():\n",
    "        folder = f\"data/processed/{key}/{split}/sequences\"\n",
    "        utils.save_sequences(data, folder, f\"seq_{split}_data_{key}.npy\", f\"seq_{split}_labels_{key}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4e61b8f-e22c-48a3-b7d7-13bcf0385925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing up memory\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
